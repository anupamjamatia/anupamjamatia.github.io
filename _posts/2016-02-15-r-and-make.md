---
title: Automating data-analysis pipelines using R and Make
subtitle: Slides and a hands-on activity
layout: post
---

[Slides](http://stat545-ubc.github.io/automation01_slides/) and a [hands-on activity](http://stat545-ubc.github.io/automation04_make-activity.html)

[![Automation: xkcd comic 1319](https://imgs.xkcd.com/comics/automation.png)](http://xkcd.com/1319/)

> 'Automating' comes from the roots 'auto-' meaning 'self-', and 'mating', meaning 'screwing'.

Bioinformatics analysis often involves designing a pipeline of commands and running that pipeline on many data sets. There are many ways to tackle this common task. Running commands interactively at the command line has the downside of being terribly unreproducible, unless one's memory is fantastically infallible. Recording the commands in a shell script certainly beats storing the commands in one's leaky brain, but is not particularly well suited to resuming the pipeline at a particular point, as is necessary after making a change to one step of the pipeline, nor to running independents steps in parallel. The venerable UNIX Make program is surprisingly well suited to describing bioinformatics pipelines. Make can resume a pipeline after a failed command without needing to start over, and it runs independent jobs in parallel. A Makefile describes a pipeline of shell commands and the interdependencies of the input and output files of those commands. A Makefile can be easily displayed as a graphical flow chart of files and shell commands, and such a visualization is a pleasing and powerful way to interpret a pipeline oneself or to communicate a pipeline to a collaborator.

See also [Open, reproducible science using Make, RMarkdown and Pandoc](http://sjackman.ca/open-science/)
